{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://s3.amazonaws.com/text-datasets/nietzsche.txt\n",
      "606208/600901 [==============================] - 2s 4us/step\n",
      "614400/600901 [==============================] - 2s 4us/step\n",
      "('Corpus length:', 600901)\n"
     ]
    }
   ],
   "source": [
    "# Downloading and parsing the initial text file\n",
    "import keras\n",
    "import numpy as np\n",
    "\n",
    "path = keras.utils.get_file(\n",
    "    'nietzsche.txt',\n",
    "    origin='https://s3.amazonaws.com/text-datasets/nietzsche.txt')\n",
    "text = open(path).read().lower()\n",
    "print('Corpus length:', len(text))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Number of sequences:', 200281)\n",
      "('Unique characters:', 59)\n",
      "Vectoriziation...\n"
     ]
    }
   ],
   "source": [
    "# Vectorizing sequences of characters\n",
    "\n",
    "maxlen = 60\n",
    "step = 3\n",
    "sentences = []\n",
    "next_chars = []\n",
    "\n",
    "for i in range(0,len(text) - maxlen, step):\n",
    "    sentences.append(text[i:i+maxlen])\n",
    "    next_chars.append(text[i+maxlen])\n",
    "    \n",
    "print('Number of sequences:', len(sentences))\n",
    "\n",
    "chars = sorted(list(set(text)))\n",
    "print('Unique characters:', len(chars))\n",
    "char_indices = dict((char, chars.index(char)) for char in chars)\n",
    "\n",
    "print('Vectoriziation...')\n",
    "x = np.zeros((len(sentences),maxlen,len(chars)), dtype=np.bool)\n",
    "y = np.zeros((len(sentences), len(chars)), dtype = np.bool)\n",
    "\n",
    "for i, sentence in enumerate(sentences):\n",
    "    for t, char in enumerate(sentence):\n",
    "        x[i, t, char_indices[char]] = 1\n",
    "    y[i, char_indices[next_chars[i]]] = 1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Building single-layer LSTM model for next-character prediction\n",
    "from keras import layers\n",
    "\n",
    "model = keras.models.Sequential()\n",
    "model.add(layers.LSTM(128, input_shape=(maxlen, len(chars))))\n",
    "model.add(layers.Dense(len(chars), activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Model compilation configuration\n",
    "optimizer = keras.optimizers.RMSprop(lr=0.01)\n",
    "model.compile(loss ='categorical_crossentropy', optimizer = optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given a trained model and a seed text snippet, new text can be generated by repeating:\n",
    "1. Draw from the model a probability distribution for the next character, given the generated text available so far\n",
    "2. Reweight the distribution to a certain temperature\n",
    "3. Sample the next character at random according to the reweighted distribution\n",
    "4. Add the new character at the end of the available text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Function to sample the next character given the model's predictions\n",
    "def sample(preds, temperature = 1.0):\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds)/temperature\n",
    "    exp_preds=np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1,preds,1)\n",
    "    return np.argmax(probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('epoch', 1)\n",
      "Epoch 1/1\n",
      "200281/200281 [==============================] - 261s 1ms/step - loss: 2.0365\n",
      "--- Generating with seed: \", not because you have deceived me, but because i can\n",
      "no lon\"\n",
      "('--- temperature: ', 0.2)\n",
      ", not because you have deceived me, but because i can\n",
      "no lon('--- temperature: ', 0.5)\n",
      ", not because you have deceived me, but because i can\n",
      "no lon('--- temperature: ', 1.0)\n",
      ", not because you have deceived me, but because i can\n",
      "no lon('--- temperature: ', 1.2)\n",
      ", not because you have deceived me, but because i can\n",
      "no longerure on natesthysser, nou\n",
      " frean upon the lerd whearnatine: which restratiea. tond, of bithingses, any\n",
      "dimal,e.\n",
      "cintle in the supriywerstind--which longer feopety\n",
      "toops,\n",
      "that m speciining in vortematy,\" bus we nout\n",
      "a\n",
      "but hapsmery, and pave ily for at preasimation!,\n",
      "thailve simpor; wretherin to the knode which last dadiminits,ably i 3adear exility his \n",
      "slive be of    napoly,o gon incamposayo, ava('epoch', 2)\n",
      "Epoch 1/1\n",
      "200281/200281 [==============================] - 250s 1ms/step - loss: 1.6671\n",
      "--- Generating with seed: \"paths ascend to the highest steeps in order to laugh to scor\"\n",
      "('--- temperature: ', 0.2)\n",
      "paths ascend to the highest steeps in order to laugh to scor('--- temperature: ', 0.5)\n",
      "paths ascend to the highest steeps in order to laugh to scor('--- temperature: ', 1.0)\n",
      "paths ascend to the highest steeps in order to laugh to scor('--- temperature: ', 1.2)\n",
      "paths ascend to the highest steeps in order to laugh to scorigion, beldemist ration: ord\n",
      "misid it abouter hence of the\n",
      "way\n",
      "iswearting asim\" the spien bavne maye? at so\n",
      "prescybe aix to\n",
      "iusimyledly one duality: up to longer ontisi, the religing who they strangencen domaetofonle. ele\n",
      "one\n",
      "jurixy be pries the extents eeperis\n",
      "\"goid word opiniinasiod\n",
      "limiting it d imisestence\n",
      "is roting\n",
      "them originimal stand, thet is curfupity one--\"ever; refitemingerfyms poste, h('epoch', 3)\n",
      "Epoch 1/1\n",
      "141568/200281 [====================>.........] - ETA: 1:18 - loss: 1.5809"
     ]
    }
   ],
   "source": [
    "# Text-generation loop\n",
    "import random\n",
    "import sys\n",
    "\n",
    "for epoch in range(1,60):\n",
    "    print('epoch', epoch)\n",
    "    model.fit(x,y,batch_size=128, epochs=1)\n",
    "    start_index = random.randint(0,len(text)-maxlen-1)\n",
    "    generated_text = text[start_index:start_index+maxlen]\n",
    "    print('--- Generating with seed: \"' + generated_text+'\"')\n",
    "    \n",
    "    for temperature in [0.2,0.5,1.0,1.2]:\n",
    "        print('--- temperature: ',temperature)\n",
    "        sys.stdout.write(generated_text)\n",
    "        \n",
    "    for i in range(400): #generating 400 characters starting from the seed text\n",
    "        sampled = np.zeros((1,maxlen, len(chars)))\n",
    "        for t, char in enumerate(generated_text):\n",
    "            sampled[0,t,char_indices[char]]=1.\n",
    "            \n",
    "        preds = model.predict(sampled,verbose=0)[0]\n",
    "        next_index=sample(preds,temperature)\n",
    "        next_char = chars[next_index]\n",
    "        \n",
    "        generated_text += next_char\n",
    "        generated_text = generated_text[1:]\n",
    "        \n",
    "        sys.stdout.write(next_char)\n",
    "                    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wrapping up\n",
    "\n",
    "It is possible to generate discrete sequence data by training a model to predict the next tokens, given the precvious tokens.\n",
    "In the case of text, such a model is called a *language model*. It can be based on either words or characters. Sampling the next token requires balance between adhering to what the model judges likely and introducing randomness.\n",
    "This can be done by softmax temperature. ! Always experiement with different temperatures to find the right one. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
